import re
from abc import ABCMeta, abstractmethod
from PIL import Image
import struct
from sklearn.metrics import jaccard_similarity_score
import os
import errno
import collections
import csv
import warnings
from datetime import datetime

"""Base class for parser, need to be implemented by each benchmark"""

class Parser():
    __metaclass__ = ABCMeta
    # error bounds for relative error analysis, default is 0%, 2% and 5%
    __errorLimits = [0.0, 2.0, 5.0]
    __keys = []
    # it will keep the first threshold key
    __firstKey = ""

    # errorsParsed
    _errors = {}
    # for relErrLowerLimit
    _relErrLowerLimit = {}
    # for localityParser2D
    _locality = {}
    # for jaccardCoefficient
    _jaccardCoefficientDict = {}

    # if the processing database is generated from a fault injection
    _isFaultInjection = False

    # this will contains the csv that dictates the if the processing log is valid or not
    _checkRunsCsv = None

    # ecc on or off
    # it is valid only for older logfiles version, the ones which does not
    # contain the ECC word in the self._logfilename
    _ecc = False

    # specific atributes for CSV write
    # log filename str which contains date, and benchmark name
    _logFileName = ""
    # str which indentify the device
    _machine = ""
    # benchmark name as string
    _benchmark = None
    # logfilename header, all special characters were replaced by -
    _header = ""
    # iteration where an sdc appeared
    _sdcIteration = -1
    # acc time when an SDC appeared
    _accIteErrors = -1
    _iteErrors = -1

    # support attributes
    # build locality images
    _buildImages = False
    # if header was already written
    _headerWritten = False
    # raw error list
    _errList = []
    # header without - separator
    _pureHeader = ""
    _logFileNameNoExt = ""
    _dirName = ""

    # size must be set on the child classes
    # size will be the name of each benchmark dir
    _size = ""

    # csv output header, stay calm, this will be write once on each csv file generated by
    _csvHeader = ["logFileName", "Machine", "Benchmark", "Header", "SDC Iteration", "#Accumulated Errors",
                  "#Iteration Errors", "Max Relative Error", "Min Rel Error",
                  "Average Rel Err", "zeroOut", "zeroGold"]

    # relative error types
    __relativeErrorTypes = ["cubic", "square", "line", "single", "random"]

    # for relativeErrorParser
    _maxRelErr = 0
    _minRelErr = 0
    _avgRelErr = 0
    _zeroOut = 0
    _zeroGold = 0

    # for benchmarks which have a third dimention this attribute must be set on the child process
    _hasThirdDimention = False

    def __init__(self, **kwargs):
        # keys must be set according to CAIO's approach
        try:
            parseForHistogram = kwargs.pop("parse_err_histogram")
        except:
            parseForHistogram = False

        # this is necessary for CAIO's approach
        if parseForHistogram:
            errorHistogram = kwargs.pop("error_histogram")
            precision = float(errorHistogram["PRECISION"])
            limitRange = errorHistogram["LIMIT_RANGE"]
            self.__errorLimits = [float(i) / precision for i in range(0, precision * limitRange + 1)]

        self.__keys = ["errorLimit" + str(i) for i in self.__errorLimits]
        self.__firstKey = self.__keys[0]

        if "relative_errors_<=_" + str(self.__errorLimits[0]) not in self._csvHeader:
            # for python list interpretation is faster than a concatenated loop
            self._csvHeader.extend("relative_errors_<=_" + str(threshold) for threshold in self.__errorLimits)
            self._csvHeader.extend("jaccard_>_" + str(threshold) for threshold in self.__errorLimits)
            self._csvHeader.extend(t + "_>" + str(threshold) for threshold in self.__errorLimits for t in self.__relativeErrorTypes)

        try:
            self._isFaultInjection = bool(kwargs.pop("is_fi"))
        except:
            self._isFaultInjection = False

        try:
            self._checkRunsCsv = kwargs.pop("check_csv")
        except:
            self._checkRunsCsv = None

        try:
            self._ecc = bool(kwargs.pop("ecc"))
        except:
            self._ecc = False

    """
    debug method, only print the class attributes
    """

    def debugAttPrint(self):
        print "*******Var values*******"
        print "log file", self._logFileName
        print "machine", self._machine
        print "acc ite errors", self._accIteErrors
        print "bencharmk", self._benchmark
        print "header", self._header
        print "sdcIterators", self._sdcIteration
        print "iteErrors", self._iteErrors
        print "size", self._size
        print "dir name", self._dirName
        print "third dimention", self._hasThirdDimention

    """
    This method will set all attributes that were first defined by Daniel on the first script
    they are very helpful, and they also are used when csv file is generated
    """

    def setDefaultValues(self, logFileName, machine, benchmark, header, sdcIteration, accIteErrors, iteErrors, errList,
                         logFileNameNoExt, pureHeader):
        self._logFileName = logFileName
        self._machine = machine
        self._benchmark = benchmark
        self._header = header
        self._sdcIteration = sdcIteration
        self._accIteErrors = accIteErrors

        self._iteErrors = iteErrors
        self._errList = errList
        # print "\n\nerr list inside " , len(errList)
        self._pureHeader = pureHeader
        self._logFileNameNoExt = logFileNameNoExt

        # self._size = \
        self.setSize(self._pureHeader)

        self._makeDirName()

        # for csv run check
        if self._checkRunsCsv:
            # for i in self._checkRunsCsv:
            board_key = str(self._machine) + ("_ecc_on" if self._ecc else '')
            # print self._checkRunsCsv[board_key]["csv"]
            csvObjFile = open(self._checkRunsCsv[board_key]["csv"])

            # to check the delimiter
            dialect = csv.Sniffer().sniff(csvObjFile.read(), delimiters=';,')
            csvObjFile.seek(0)
            readerTwo = csv.reader(csvObjFile, dialect=dialect)
            self._checkRunsCsv[board_key]["data"] = [j for j in readerTwo]
            csvObjFile.close()
            # ----------------

    """
    some benchmarks have a third dimention
    so this will return _hasThirdDimention
    """

    def getHasThirdDimention(self):
        return self._hasThirdDimention

    """
    call to the private method paseerrMethod
    for each errString in _errList
    """

    def parseErr(self):
        self._errors[self.__firstKey] = []
        for errString in self._errList:
            if self._isLogValid:
                err = self.parseErrMethod(errString)
                if err is not None:
                    self._errors[self.__firstKey].append(err)

    """
    _relativeErrorParser caller, if you want override _relativeErrorParse
     only put all errors on self._errors[self.__firstKey] so
    it will be parsed by your _relativeErrorParse
    """

    def relativeErrorParser(self):
        self._relativeErrorParser(self._errors[self.__firstKey])

    @abstractmethod
    def parseErrMethod(self, errString):
        raise NotImplementedError()

    """
        build image, based on object parameters
    """

    @abstractmethod
    def buildImageMethod(self, *args):
        raise NotImplementedError()

    """
    this method is very important, it must set self._size attribute
    with a string that contains a setup configuration
    for example: I have a dgemm with 2048 x 2048
    so my self._size = str(size_m_size_n),
    where m == 2048 and n == 2048
    """

    @abstractmethod
    def setSize(self, header):
        raise NotImplementedError()

    """
    relative error with a generic range of tolerated values
    input:
    relError: error found in the SDC
    err: list of elements in the locality classification

    """

    def __placeRelativeError(self, relError, err):
        # if relError < self._toleratedRelErr:
        #     relErrLowerLimit += 1
        # else:
        #     errListFiltered.append(err)
        # if relError < self._toleratedRelErr2:
        #     relErrLowerLimit2 += 1
        # else:
        #     errListFiltered2.append(err)
        for key, threshold in zip(self.__keys, self.__errorLimits):
            if relError < threshold:
                self._relErrLowerLimit[key] += 1
            else:
                self._errors[key].append(err)

    """
    to clean all relative errors attributes for the class
    attributes to be cleaned:
    relErrLowerLimit
    _errors = {}
    _relErrLowerLimit = {}
    """

    def __cleanRelativeErrorAttributes(self):
        for key in self.__keys:
            # to store all error parsed values
            self._errors[key] = []
            self._relErrLowerLimit[key] = 0

    """
    if you want other relative error parser this method must be override
    return [highest relative error, lowest relative error, average relative error,
    # zeros in the output,
    #zero in the GOLD,
    #errors with relative errors lower than limit(toleratedRelErr) for each _errorLimits value,
    """

    def _relativeErrorParser(self, errList):
        relErr = []
        zeroGold = 0
        zeroOut = 0
        self.__cleanRelativeErrorAttributes()

        for err in errList:
            read = float(err[2])
            expected = float(err[3])
            absoluteErr = abs(expected - read)
            if abs(read) < 1e-6:
                zeroOut += 1
            if abs(expected) < 1e-6:
                zeroGold += 1
            else:
                relError = abs(absoluteErr / expected) * 100
                relErr.append(relError)
                # generic way to parse for many error threshold
                self.__placeRelativeError(relError, err)

        if len(relErr) > 0:
            self._maxRelErr = max(relErr)
            self._minRelErr = min(relErr)
            self._avgRelErr = sum(relErr) / float(len(relErr))

        self._zeroOut = zeroOut
        self._zeroGold = zeroGold

    """
    jaccardCoefficient caller method
    this method only calls _jaccardCoefficient method, once _jaccardCoefficient could
    be implemented for many benchmarks
    """

    def jaccardCoefficient(self):
        with warnings.catch_warnings():
            warnings.simplefilter("ignore", category=RuntimeWarning)
            for keys, values in self._errors.iteritems():
                self._jaccardCoefficientDict[keys] = self._jaccardCoefficient(values)

    """
    jaccardCoefficient parser. This method could be overwritten if necessary
    input:
    errListJaccard: list of errors
    """

    def _jaccardCoefficient(self, errListJaccard):
        expected = []
        read = []
        for err in errListJaccard:
            try:
                readGStr = ''.join(bin(ord(c)).replace('0b', '').rjust(8, '0') for c in struct.pack('!f', err[2]))
                expectedGStr = ''.join(
                    bin(ord(c)).replace('0b', '').rjust(8, '0') for c in struct.pack('!f', err[3]))
            except OverflowError:
                readGStr = ''.join(bin(ord(c)).replace('0b', '').rjust(8, '0') for c in struct.pack('!d', err[2]))
                expectedGStr = ''.join(
                    bin(ord(c)).replace('0b', '').rjust(8, '0') for c in struct.pack('!d', err[3]))

            read.extend([n for n in readGStr])
            expected.extend([n for n in expectedGStr])

        try:
            jac = jaccard_similarity_score(expected, read)
            dissimilarity = float(1.0 - jac)
            return dissimilarity
        except:
            return None

    """
    locality caller method 2d, and 3d if it's avaliable
    """

    def localityParser(self):
        with warnings.catch_warnings():
            warnings.simplefilter("ignore", category=RuntimeWarning)
            print "\n"
            for key, value in self._errors.iteritems():
                print key
                if self._hasThirdDimention:
                    self._locality[key] = self._localityParser3D(value)
                else:
                    self._locality[key] = self._localityParser2D(value)

    """
    locality parser for 2d benchmarks
    input:
    errList: list of errors parsed by parseErr
    """

    def _localityParser2D(self, errList):
        if len(errList) < 1:
            return [0, 0, 0, 0, 0]
        elif len(errList) == 1:
            return [0, 0, 0, 1, 0]
        else:
            allXPositions = [x[0] for x in errList]  # Get all positions of X
            allYPositions = [x[1] for x in errList]  # Get all positions of Y
            counterXPositions = collections.Counter(allXPositions)  # Count how many times each value is in the list
            counterYPositions = collections.Counter(allYPositions)  # Count how many times each value is in the list
            rowError = any(
                x > 1 for x in counterXPositions.values())  # Check if any value is in the list more than one time
            colError = any(
                x > 1 for x in counterYPositions.values())  # Check if any value is in the list more than one time
            if rowError and colError:  # square error
                return [0, 1, 0, 0, 0]
            elif rowError or colError:  # row/col error
                return [0, 0, 1, 0, 0]
            else:  # random error
                return [0, 0, 0, 0, 1]

    """
    locality parser for 3d benchmarks
    input:
    errList: list of errors parsed by parseErr
    """

    def _localityParser3D(self, errList):
        if len(errList) < 1:
            return [0, 0, 0, 0, 0]
        elif len(errList) == 1:
            return [0, 0, 0, 1, 0]
        else:
            allXPositions = [x[0] for x in errList]  # Get all positions of X
            allYPositions = [x[1] for x in errList]  # Get all positions of Y
            allZPositions = [x[2] for x in errList]  # Get all positions of Y
            counterXPositions = collections.Counter(allXPositions)  # Count how many times each value is in the list
            counterYPositions = collections.Counter(allYPositions)  # Count how many times each value is in the list
            counterZPositions = collections.Counter(allZPositions)  # Count how many times each value is in the list
            rowError = any(
                x > 1 for x in counterXPositions.values())  # Check if any value is in the list more than one time
            colError = any(
                x > 1 for x in counterYPositions.values())  # Check if any value is in the list more than one time
            heightError = any(
                x > 1 for x in counterZPositions.values())  # Check if any value is in the list more than one time
            if rowError and colError and heightError:  # cubic error
                return [1, 0, 0, 0, 0]
            if (rowError and colError) or (rowError and heightError) or (heightError and colError):  # square error
                return [0, 1, 0, 0, 0]
            elif rowError or colError or heightError:  # line error
                return [0, 0, 1, 0, 0]
            else:  # random error
                return [0, 0, 0, 0, 1]

    """
    public caller method to write csv
    only calls _writeToCSV method
    """

    def writeToCSV(self):
        with warnings.catch_warnings():
            warnings.simplefilter("ignore", category=RuntimeWarning)
            output = self._dirName + "/logs_parsed_" + self._machine + ".csv"
            self._writeToCSV(output)

    """
    write a list as a row to CSV
    if you want other type of write to csv,
    the method __writeToCSV and atribute __csvHeader must be changed
    input:
    csvFilename: output filename for csv, this will be generated using what is produced by setSize method
    """

    def _writeToCSV(self, csvFileName):
        if os.path.isfile(csvFileName) == False:
            self._writeCSVHeader(csvFileName)
            self._headerWritten = True

        try:
            csvWFP = open(csvFileName, "a")
            writer = csv.writer(csvWFP, delimiter=';')
            outputList = [self._logFileName,
                          self._machine,
                          self._benchmark,
                          self._header,
                          self._sdcIteration,
                          self._accIteErrors,
                          self._iteErrors,
                          self._maxRelErr,
                          self._minRelErr,
                          self._avgRelErr,
                          self._zeroOut,
                          self._zeroGold]

            outputList.extend(self._relErrLowerLimit[key] for key in self.__keys)
            outputList.extend(self._jaccardCoefficientDict[key] for key in self.__keys)
            for key in self.__keys:
                outputList.extend(self._locality[key])

            writer.writerow(outputList)
            csvWFP.close()

        except:
            # ValueError.message += ValueError.message + "Error on writing row to " + str(csvFileName)
            print "Error on writing row to " + str(csvFileName)
            raise

    """
    writes a csv header, and create the log_parsed directory
    input:
    csvFilename: output filename for csv, this will be generated using what is produced by setSize method
    """

    def _writeCSVHeader(self, csvFileName):
        if not os.path.isfile(csvFileName):
            if not os.path.exists(os.path.dirname(csvFileName)):
                try:
                    os.makedirs(os.path.dirname(csvFileName))
                except OSError as exc:  # Guard against race condition
                    if exc.errno != errno.EEXIST:
                        raise

            csvWFP = open(csvFileName, "a")
            writer = csv.writer(csvWFP, delimiter=';')
            writer.writerow(self._csvHeader)
            csvWFP.close()

    """
    makes the csv output directory, for each config, for each Device, for each size
    this method will use self._size attribute which is produced by setSize method
    """

    def _makeDirName(self):
        self._dirName = os.getcwd() + "/" + self._machine + "/" + self._benchmark + "/" + str(self._size) + "/"
        if not os.path.exists(os.path.dirname(self._dirName)):
            try:
                os.makedirs(os.path.dirname(self._dirName))
            except OSError as exc:  # Guard against race condition
                if exc.errno != errno.EEXIST:
                    raise

    """
    this method check if the processing log is on a valid
    radiation test run
    if self._isFaultInjection is True this method will always return true
    """

    @property
    def _isLogValid(self):
        if self._isFaultInjection or self._checkRunsCsv is None:
            return True
        board_key = str(self._machine) + ("_ecc_on" if self._ecc else '')
        currentData = self._checkRunsCsv[board_key]["data"]
        # process data
        # 2016_12_13_19_00_34_cudaDarknet_carol-k402.log
        m = re.match("(\d+)_(\d+)_(\d+)_(\d+)_(\d+)_(\d+)_(.*)_(.*).log", self._logFileName)
        if m:
            year = m.group(1)
            month = m.group(2)
            day = m.group(3)
            hour = m.group(4)
            minutes = m.group(5)
            second = m.group(6)

            # assuming microsecond = 0
            currDate = datetime(int(year), int(month), int(day), int(hour), int(minutes), int(second))
            for j in currentData:
                # startDate = j["start timestamp"]
                # endDate = j["end timestamp"]
                # doing it I can use daniel raw summaries-fission.csv file
                try:
                    startDate = j[0]
                    endDate = j[1]
                    validBench = j[2] in self._benchmark or self._benchmark in j[2]

                    startDate = datetime.strptime(startDate, "%c")
                    endDate = datetime.strptime(endDate, "%c")
                    if startDate <= currDate <= endDate and validBench:
                        # print "\nstart date ", startDate , " enddate ", endDate, " currDate ", currDate
                        return True
                except:
                    pass

        return False

    """
    buildImage method, will build error caracterization for an specific benchmark
    this method is always called. If no images will be generate the method will only
    contains pass command
    :param
    errors: error list
    size: size of the output produced by your benchmark
    filename: filename to draw an image
    """

    def _buildImage(self, errors, size, filename):
        # identifica em qual posicao da matriz ocorreram os erros
        # definindo as bordas [esquerda, cabeca, direita, pe]
        err_limits = [int(size), int(size), 0, 0]
        for error in errors:
            if int(error[0]) < err_limits[0]:
                err_limits[0] = int(error[0])
            if int(error[0]) > err_limits[2]:
                err_limits[2] = int(error[0])
            if int(error[1]) < err_limits[1]:
                err_limits[1] = int(error[1])
            if int(error[1]) > err_limits[3]:
                err_limits[3] = int(error[1])

        # adiciona 5 pontos em cada lado para visualizacao facilitada
        # verifica que isso nao ultrapassa os limites da matriz
        err_limits[0] -= 5
        err_limits[1] -= 5
        err_limits[2] += 5
        err_limits[3] += 5
        if err_limits[0] < 0:
            err_limits[0] = 0
        if err_limits[1] < 0:
            err_limits[1] = 0
        if err_limits[2] > size:
            err_limits[2] = size
        if err_limits[3] > size:
            err_limits[3] = size

        # define uma imagem com o dobro do tamanho, para poder adicionar as guias
        # (o quadriculado)
        size_x = (err_limits[2] - err_limits[0]) * 2 + 1
        size_y = (err_limits[3] - err_limits[1]) * 2 + 1
        img = Image.new("RGB", (size_x, size_y), "white")

        n = 0

        # adiciona os erros a imagem
        for error in errors:
            n += 1
            try:
                if (n < 499):
                    img.putpixel(((int(error[0]) - err_limits[0]) * 2, (int(error[1]) - err_limits[1]) * 2),
                                 (255, 0, 0))
                else:
                    img.putpixel(((int(error[0]) - err_limits[0]) * 2, (int(error[1]) - err_limits[1]) * 2),
                                 (0, 0, 255))
            except IndexError:
                print ("Index error: ", error[0], ";", err_limits[0], ";", error[1], ";", err_limits[1])

        # adiciona as guias (quadriculado)
        if (size_x < 512) and (size_y < 512):
            for y in range(size_y):
                for x in range(size_x):
                    if (x % 2) == 1 or (y % 2) == 1:
                        img.putpixel((x, y), (240, 240, 240))

        if not os.path.exists(os.path.dirname(filename)):
            try:
                os.makedirs(os.path.dirname(filename))
            except OSError as exc:  # Guard against race condition
                if exc.errno != errno.EEXIST:
                    raise

        img.save(filename + '.png')

    """
    LEGACY METHODS SECTION
    """

    """
    legacy method
    """
    # def _setCheckRunsCsvsAndOpen(self, csvSummaries):
    #     if self._isFaultInjection or csvSummaries == None:
    #         return
    #     else:
    #         self._checkRunsCsv = csvSummaries

    """legacy method"""
    # @abstractmethod
    # def getBenchmark(self):
    #     raise NotImplementedError

    """legacy method"""
    # def setBuildImage(self, val):
    #     self.__buildImage = True

    """legacy method"""
    # """
    # if the csvHeader must be different,
    # the variable must be set to the other value,
    # so getCSVHeader will return other constant
    # """
    #
    # def getCSVHeader(self):
    #     return self._csvHeader

    """
    legacy method
    """
    # def getImageIndex(self):
    #     return self._imageIndex

    """
    legacy method
    """
    # def setImageIndex(self, imageIndex):
    #     self._imageIndex = imageIndex
